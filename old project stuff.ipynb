{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old OLS solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from random import random, seed\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class myOLS :\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.beta = 0\n",
    "    \n",
    "    def train(self, xb, y):\n",
    "        self.beta = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)\n",
    "    \n",
    "    def predict(self, xb):\n",
    "        return xb.dot(self.beta)\n",
    "\n",
    "# Given two column vecters x,y, return an array with columns x, y, x^2, xy, y^2, ... y^n \n",
    "def productlist(x, y, n):\n",
    "    pl = np.concatenate([(np.power(x,k-i)*np.power(y,i)) for k in range(1, n+1) for i in range(0,k+1)], axis=1)\n",
    "    return pl\n",
    "    \n",
    "class polynominalOLS :\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.polyOLS = myOLS()\n",
    "        self.degree = n\n",
    "    \n",
    "    # Train our polynomial OLS.\n",
    "    # We take a list of xs and ys with given zs as output. \n",
    "    # First we use product list to build all products of the form x^k y^l with k + l <= degree\n",
    "    # Put in ones and then feed that as the training date to an ordinary OLS\n",
    "    def train(self,x,y,z):\n",
    "        pl = productlist(x,y, self.degree)\n",
    "        xb = np.concatenate([np.ones((len(x), 1)), pl], axis=1)\n",
    "        self.polyOLS.train(xb,z)\n",
    "    \n",
    "    # Predict our polynomial solution \n",
    "    # As above, take only xs and ys.\n",
    "    # Start by making a list of products in the polynomial\n",
    "    # Then use that as input to predict on an ordinart OLS\n",
    "    def predict(self,x,y):\n",
    "        pl = productlist(x,y, self.degree)\n",
    "        xb = np.concatenate([np.ones((1, len(x))), pl], axis=1)\n",
    "        return self.polyOLS.predict(xb)\n",
    "    \n",
    "    # This is only used for plotting. \n",
    "    # It is just a wrapper function to predict the value a single (x,y) pair\n",
    "    def plotfunc(self, x, y):\n",
    "        return self.predict(np.c_[x],np.c_[y])\n",
    "        \n",
    "    def coefficients(self):\n",
    "        return self.polyOLS.beta\n",
    "    \n",
    "def FrankeFunction(x,y):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "def otherFunction(x,y) :\n",
    "    return x*y\n",
    "\n",
    "# Setup input data. \n",
    "# We make numberofpoints points\n",
    "# the x's and y's are chosen randomly \n",
    "# the z's according to the fomula and with some noise\n",
    "numberofpoints = 100\n",
    "noise = 0.2\n",
    "x = np.random.rand(numberofpoints, 1) # a column vector with numberofpoints entries \n",
    "y = np.random.rand(numberofpoints, 1)\n",
    "\n",
    "# we have to flip the normal dist array to get the right dimensions \n",
    "z = FrankeFunction(x,y) + np.c_[np.random.normal(0,0.2,numberofpoints)] \n",
    "#z = otherFunction(x,y) + np.c_[noise*np.random.normal(0,1,numberofpoints)]\n",
    "\n",
    "pOLS = polynominalOLS(5)\n",
    "pOLS.train(x,y,z)\n",
    "\n",
    "## ploting \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make plot points .\n",
    "xpoints = np.arange(0, 1, 0.05)\n",
    "ypoints = np.arange(0, 1, 0.05)\n",
    "xm, ym = np.meshgrid(xpoints,ypoints)\n",
    "\n",
    "# Plot the real and predicted surfaces.\n",
    "\n",
    "realsurf = ax.plot_surface(xm, ym, FrankeFunction(xm, ym), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "#realsurf = ax.plot_surface(xm, ym, otherFunction(xm, ym), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "predictedsurf = ax.plot_surface(xm, ym, np.vectorize(pOLS.plotfunc)(xm,ym), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-0.10, 1.40)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "#fig.colorbar(realsurf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pOLS.coefficients()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does k-fold cross validation \n",
    "# It takes a k, a matrix XY with two columns of inputs, and corresponding true values z (all of appropirate sizes)\n",
    "# It also takes a model which we assume have train, predict, and coefficients functions.\n",
    "# Both should work on these matrices \n",
    "# The final argument is a loss function, that should act on two variables\n",
    "# We return the expected error, and a list of computed coefficients for our model\n",
    "def cross_validation(k, XY, z, model, loss):\n",
    "    CVsum = 0 # our estimatet error, not devided by size (Hastings et al (7.48)*N)\n",
    "    betas = [] # list of coefficients \n",
    "    \n",
    "    clength = len(XY[:,0]) # the length of the first column in XY\n",
    "    permutation = np.random.permutation(clength) # a permutation of the indexes of rows in XY\n",
    "    partitions = np.array_split(permutation, k) # The permutation is devided in to k almost equally big groups\n",
    "    \n",
    "    for i in range(0,k) :\n",
    "        # create a mask to pick everything but the elements in the i'th partition\n",
    "        mask = np.ones(clength,dtype=bool)\n",
    "        mask[partitions[i]] = 0\n",
    "        # now train on everything but the i'th partition\n",
    "        model.train(XY[mask, :], z[mask, :])\n",
    "        # add the coefficients to betas\n",
    "        betas.append(model.coefficients())\n",
    "        # make the mask the picks only the elements of the i'th partition\n",
    "        notmask = np.invert(mask)\n",
    "        # update the error with the loss function of our predicted values and the true values\n",
    "        # all in the i'th partition\n",
    "        CVsum = CVsum + loss(model.predict(XY[notmask, :]), z[notmask, :]) \n",
    "    \n",
    "    CVerr = CVsum / clength # devide by len(xs) to get the expression from (7.48)\n",
    "    return CVerr, betas\n",
    "\n",
    "# take two column vectors, compute the sum of the squares of the difference at each entry \n",
    "def sqdiff(xs,ys) :\n",
    "    return np.sum(np.power(xs - ys, 2))\n",
    "\n",
    "# We now test, using 10 fold cross-valdidation, which degree polynomial 1,2,3,4, or 5, gives the best model\n",
    "for degree in range(1,6) :\n",
    "    pOLS = polynomialOLS(degree)\n",
    "    print(f\"Using 10 fold cross validation for a polynomial of degree {degree}.\")\n",
    "    cv, betas = cross_validation(10, XY, z, pOLS, sqdiff)\n",
    "    print(\"The predicted error is %.4f\" % cv)\n",
    "    print(\"The variance on the betas are:\")\n",
    "    # To compute the variance, we first convert our list of column vectors into a single matrix \n",
    "    # We accomplish this by using hstack \n",
    "    # Then we use apply_along_axis to apply the function variance to each row \n",
    "    print(np.apply_along_axis(variance, 1, np.hstack(betas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pOLS = polynominalOLS(5)\n",
    "pOLS.train(XY,z)\n",
    "\n",
    "## ploting \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make plot points .\n",
    "xpoints = np.arange(0, 1, 0.05)\n",
    "ypoints = np.arange(0, 1, 0.05)\n",
    "xm, ym = np.meshgrid(xpoints,ypoints)\n",
    "\n",
    "# Plot the real and predicted surfaces.\n",
    "realsurf = ax.plot_surface(xm, ym, FrankeFunction(xm, ym), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "predictedsurf = ax.plot_surface(xm, ym, np.vectorize(pOLS.plotfunc)(xm,ym), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-0.10, 1.40)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "#fig.colorbar(realsurf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pOLS.coefficients()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
